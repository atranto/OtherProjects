---
title: "Chap 7: Deep-Regression for forecasting"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data Loading
```{r}
# read the data
#setwd("/Users/alex/Dropbox/teaching/ST3233_2018/dataset/")
setwd("/home/alekthiery/Dropbox/teaching/ST3233_2018/dataset")
fname = "jena_climate_2009_2016.csv"
jena_raw <- read.csv(fname)

# data is every 10min. Keep only hourly data
jena_hourly = jena_raw[seq(1, nrow(jena_raw), 6), seq(1,ncol(jena_raw))]  #hourly data
```

When using Neural Network, it is always useful to make sure that the data is normalized.
```{r}
# renormalize by substracting mean and divide by std
for( k in c(2:15) ){
  jena_hourly[,k] = (jena_hourly[,k] - mean(jena_hourly[,k])) / sd(jena_hourly[,k])
}

#plot( ts(jena_hourly[1:240,"T"], frequency = 24), lwd=3, 
#      xlab="Day", ylab="Temp.", main="Temperature in Jena")
#plot( ts(jena_hourly[,"H2OC"], frequency = 24), lwd=3, 
#      xlab="Day", ylab="H2OC.", main="H2OC in Jena")
```

Let us put the data in a usable format.
```{r}
# extract the day of the year and hour of the day
# and renormalize to make it 0<x<1
suppressMessages(library(lubridate))  # for data manipulations
suppressMessages(library(dplyr))      # for data wrangling
jena_hourly$Date <- dmy_hms(jena_hourly$Date)
jena_hourly$yday <- (yday(jena_hourly$Date)-1) / 365. 
jena_hourly$hour <- hour(jena_hourly$Date) / 24.
jena_hourly <- jena_hourly %>% select(-Date)
```

Let us look at the different covariates.
```{r}
# correlation plot
suppressMessages(library(corrplot))
clag = 12
corr_mat = cor(jena_hourly[(1+clag):(10000+clag),] - jena_hourly[1:10000,])
corrplot(corr_mat, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)

# optional:: get rid of uncorrelated covariates
jena_hourly <- jena_hourly %>% select(-max, -wv, -presurre)
```

# Baseline model: persistence model
```{r}
n_max = 70000
naive_pred_accuracy  = c()
for(lag in c(1:60)){
  MAE = mean(abs(jena_hourly[1:n_max,"T"] - jena_hourly[(1+lag):(n_max+lag),"T"]))
  naive_pred_accuracy = c(naive_pred_accuracy, MAE)
}

plot(c(1:60), naive_pred_accuracy, pch=20, 
     type="b", xlab="LAG", ylab="MAE", main="PERSISTENCE MODEL: ACCURACY")
```

# First model

Only use the history of the temperature to forecast the temperature 12 hours ahead of time.

```{r}
# create the covariate and response
suppressMessages(library(data.table))  #for shift function
lag = 24*2
x_data = array(0, c(nrow(jena_hourly), lag))
x_data[,1] = jena_hourly[,"T"]
for(k in c(1:(lag-1))){
  x_data[,k+1] = data.table::shift(jena_hourly[,"T"], n=k, type="lag")
}

# forecast 12h ahead of time
y_data = data.table::shift(jena_hourly[,"T"], n=12, type="lead")

#crop to avoid NA values
x_data = x_data[(lag+1):(nrow(x_data)-200),]
y_data = y_data[(lag+1):(nrow(x_data)-200)]
```


Define the basic neural network
```{r}
library(keras)
input =  layer_input(shape = c(ncol(x_data)))
output = input %>% 
  layer_dense(units = 10, activation = "elu") %>%
  layer_dense(units = 10, activation = "elu") %>%
  layer_dense(units = 1)
model = keras_model(inputs=input, outputs = output)
```

We will use the Mean Absolute Error (MAE) as metric of accuracy. We could have chosen MSE.

```{r}
model %>% compile(
  loss = "MAE",
  optimizer = optimizer_adam(),
  metrics = c("MAE")
)

n_train = as.integer(0.7 * length(y_data))
n_total = length(y_data)
history <- model %>% fit(
  x = x_data[1:n_train,], 
  y = y_data[1:n_train], 
  epochs = 100, batch_size = 2048, 
  validation_data = list(x_data[n_train:n_total,], 
                         y_data[n_train:n_total]),
  verbose = FALSE
)

plot(history$metrics$val_loss, type="l", main="Validation Loss")
```


# Analyse of the periodic patterns

we can try to predict all the covariate from the day/hour only.
```{r}
n_train = as.integer(0.7 * nrow(jena_hourly))
n_total = nrow(jena_hourly)

hour_ts = array(0, dim=c(n_total,2))
hour_ts[,1] = sin( 2*pi*jena_hourly[,"hour"])
hour_ts[,2] = cos( 2*pi*jena_hourly[,"hour"])

yday_ts = array(0, dim=c(n_total,2))
yday_ts[,1] = sin( 2*pi*jena_hourly[,"yday"])
yday_ts[,2] = cos( 2*pi*jena_hourly[,"yday"])

response_ts = jena_hourly %>% select(-yday, -hour)
dim_response = ncol(response_ts)
response_ts = data.matrix(response_ts)
```

Define the network.
```{r}
input_hour <- layer_input(shape = c(2))
output_hour <-input_hour %>% 
  layer_dense(units = 10, activation = "elu") %>% 
  layer_dense(units = 10, activation = "elu") %>%
  layer_dense(units = dim_response) 

input_yday <- layer_input(shape = c(2))
output_yday <-input_yday %>% 
  layer_dense(units = 10, activation = "elu") %>% 
  layer_dense(units = 10, activation = "elu") %>%
  layer_dense(units = dim_response) 

output_time <- layer_add(c(output_yday, output_hour))
model_time <- keras_model(inputs = c(input_yday, input_hour), 
                          outputs = output_time)

model_yday <- keras_model(inputs = input_yday, outputs = output_yday)
```

Compile and fit!
```{r}
model_time %>% compile(
  loss = "MAE",
  optimizer = optimizer_adam(),
  metrics = c("MAE")
)

history <- model_time %>% fit(
  x=list(yday_ts[1:n_train,], hour_ts[1:n_train,]), 
  y=response_ts[1:n_train,], 
  epochs = 50, batch_size = 2048, 
  validation_data = list( list(yday_ts[n_train:n_total,], hour_ts[n_train:n_total,]), 
                          response_ts[n_train:n_total,]),
  verbose = FALSE)
plot(history$metrics$val_loss, type="l", main="Validation Loss")

```
```{r}
pred_time = model_time %>% predict( list(yday_ts, hour_ts))
pred_yday = model_yday %>% predict( yday_ts)

date_list = seq(1,70000,30)
plot(response_ts[date_list,1], type="l")
points(pred_yday[date_list,1], type="l", col="red", lwd=5,
       xlab = "Day", ylab="Temperature")

plot(pred_time[1:(24*90),1], type="l", col="red", lwd=1,
       xlab = "Day", ylab="Temperature")
```

# More fancy model

Let us now build a model that leverage the covariates, as well as the periodic patterns, as well as two additional covariates. We will add as covariates:
1. the mean of the temperature over the last day   
2. the mean of the temperature over the last 10 days  



```{r}
#data = array(0, dim=c(n_total,4+1+1+1))
k_start = 300
k_end = nrow(response_ts)

data = data.table::shift(response_ts[,1], n=12, type="lead")[k_start:k_end]
data = array(data )
data = cbind(data, 
             yday_ts[k_start:k_end,], 
             hour_ts[k_start:k_end,],
             data.matrix(response_ts[k_start:k_end,]),
             data.matrix(response_ts[(k_start-12):(k_end-12),]),
             stats::filter(response_ts[,"T"], filter=rep(1,24)/24., side=1)[k_start:k_end],
             stats::filter(response_ts[,"T"], filter=rep(1,240)/240., side=1)[k_start:k_end]
             )
data = na.omit(data)  #get rid of row with NAs

y_array = data[,1]
x_array = data[,2:ncol(data)]

yday_array = data.matrix(data[,2:3])
hour_array = data.matrix(data[,4:5])
```

Let us now define the neural network.
```{r}
input_hour <- layer_input(shape = c(2))
output_hour <-input_hour %>% 
  layer_dense(units = 10, activation = "elu") %>% 
  layer_dense(units = 10, activation = "elu") %>%
  layer_dense(units = 1) 

input_yday <- layer_input(shape = c(2))
output_yday <-input_yday %>% 
  layer_dense(units = 10, activation = "elu") %>% 
  layer_dense(units = 10, activation = "elu") %>%
  layer_dense(units = 1) 


input_cov =  layer_input(shape = c(ncol(x_array)))
output_cov = input_cov %>% 
  layer_dense(units = 10, activation = "elu") %>%
  layer_dense(units = 10, activation = "elu") %>%
  layer_dense(units = 1)

output_final = layer_add(c(output_yday, output_hour, output_cov))
#output_final = layer_concatenate(c(output_yday, output_hour, output_cov)) %>%
#  layer_dense(units = 10, activation = "elu") %>%
#  layer_dense(units = 10, activation = "elu") %>%
#  layer_dense(units = 1)
```

Compile and fit!
```{r}
model_final <- keras_model(inputs = c(input_yday, input_hour, input_cov), 
                           outputs = output_final)
model_final %>% summary()

model_final %>% compile(
  loss = "MAE",
  optimizer = optimizer_adam(),
  metrics = c("MAE")
)

n_train = as.integer(0.7*nrow(x_array))
n_total = nrow(x_array)
history = model_final %>% fit(
  x=list(yday_array[1:n_train,], hour_array[1:n_train,], x_array[1:n_train,]),
  y=y_array[1:n_train],
  epochs = 200, batch_size = 2048,
  validation_data = list(
    list(yday_array[n_train:n_total,], hour_array[n_train:n_total,], x_array[n_train:n_total,]),
    y_array[n_train:n_total]),
  verbose = FALSE)

plot(history$metrics$val_loss, type="l", main="Validation Loss")


pred_final = model_final %>% predict(list(yday_array, 
                                          hour_array, 
                                          x_array))

plot(y_array[1:500], type="l", lwd=3)
points(pred_final[1:500], type="l", col="red", lwd=3)

plot(y_array[1:10000], pred_final[1:10000], xlim=c(-4,4), ylim=c(-4,4))
abline(0,1, col="red", lwd=5)


MAE = mean(abs(y_array[n_train:n_total] - pred_final[n_train:n_total]))
cat("Validation MAE =", MAE, "\n")
```

